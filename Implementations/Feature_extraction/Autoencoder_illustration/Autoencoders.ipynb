{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Autoencoders.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "YRZxUJpg9xB0",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install magenta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nDaEgrFnSFcr",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#https://github.com/GjjvdBurg/TensorFlowExperiments/tree/master/autoencoder\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from google.colab import files\n",
        "\n",
        "from magenta.models.image_stylization.image_utils import form_image_grid\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "BATCH_SIZE = 50\n",
        "GRID_ROWS = 5\n",
        "GRID_COLS = 10\n",
        "USE_RELU = False\n",
        "\n",
        "\n",
        "def weight_variable(shape):\n",
        "    # From the mnist tutorial\n",
        "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "\n",
        "def bias_variable(shape):\n",
        "    initial = tf.constant(0.1, shape=shape)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "\n",
        "def fc_layer(previous, input_size, output_size):\n",
        "    W = weight_variable([input_size, output_size])\n",
        "    b = bias_variable([output_size])\n",
        "    return tf.matmul(previous, W) + b\n",
        "\n",
        "\n",
        "def autoencoder(x):\n",
        "    # first fully connected layer with 50 neurons using tanh activation\n",
        "    l1 = tf.nn.tanh(fc_layer(x, 28*28, 50))\n",
        "    # second fully connected layer with 50 neurons using tanh activation\n",
        "    l2 = tf.nn.tanh(fc_layer(l1, 50, 50))\n",
        "    # third fully connected layer with 2 neurons\n",
        "    l3 = fc_layer(l2, 50, 2)\n",
        "    # fourth fully connected layer with 50 neurons and tanh activation\n",
        "    l4 = tf.nn.tanh(fc_layer(l3, 2, 50))\n",
        "    # fifth fully connected layer with 50 neurons and tanh activation\n",
        "    l5 = tf.nn.tanh(fc_layer(l4, 50, 50))\n",
        "    # readout layer\n",
        "    if USE_RELU:\n",
        "        out = tf.nn.relu(fc_layer(l5, 50, 28*28))\n",
        "    else:\n",
        "        out = fc_layer(l5, 50, 28*28)\n",
        "    # let's use an l2 loss on the output image\n",
        "    loss = tf.reduce_mean(tf.squared_difference(x, out))\n",
        "    return loss, out, l3\n",
        "\n",
        "\n",
        "def layer_grid_summary(name, var, image_dims):\n",
        "    prod = np.prod(image_dims)\n",
        "    grid = form_image_grid(tf.reshape(var, [BATCH_SIZE, prod]), [GRID_ROWS, \n",
        "        GRID_COLS], image_dims, 1)\n",
        "    return tf.summary.image(name, grid)\n",
        "\n",
        "\n",
        "def create_summaries(loss, x, latent, output):\n",
        "    writer = tf.summary.FileWriter(\"./logs\")\n",
        "    tf.summary.scalar(\"Loss\", loss)\n",
        "    layer_grid_summary(\"Input\", x, [28, 28])\n",
        "    layer_grid_summary(\"Encoder\", latent, [2, 1])\n",
        "    layer_grid_summary(\"Output\", output, [28, 28])\n",
        "    return writer, tf.summary.merge_all()\n",
        "\n",
        "\n",
        "def make_image(name, var, image_dims):\n",
        "    prod = np.prod(image_dims)\n",
        "    grid = form_image_grid(tf.reshape(var, [BATCH_SIZE, prod]), [GRID_ROWS, \n",
        "        GRID_COLS], image_dims, 1)\n",
        "    s_grid = tf.squeeze(grid, axis=0)\n",
        "\n",
        "    # This reproduces the code in: tensorflow/core/kernels/summary_image_op.cc\n",
        "    im_min = tf.reduce_min(s_grid)\n",
        "    im_max = tf.reduce_max(s_grid)\n",
        "\n",
        "    kZeroThreshold = tf.constant(1e-6)\n",
        "    max_val = tf.maximum(tf.abs(im_min), tf.abs(im_max))\n",
        "\n",
        "    offset = tf.cond(\n",
        "            im_min < tf.constant(0.0),\n",
        "            lambda: tf.constant(128.0),\n",
        "            lambda: tf.constant(0.0)\n",
        "            )\n",
        "    scale = tf.cond(\n",
        "            im_min < tf.constant(0.0),\n",
        "            lambda: tf.cond(\n",
        "                max_val < kZeroThreshold,\n",
        "                lambda: tf.constant(0.0),\n",
        "                lambda: tf.div(127.0, max_val)\n",
        "                ),\n",
        "            lambda: tf.cond(\n",
        "                im_max < kZeroThreshold,\n",
        "                lambda: tf.constant(0.0),\n",
        "                lambda: tf.div(255.0, im_max)\n",
        "                )\n",
        "            )\n",
        "    s_grid = tf.cast(tf.add(tf.multiply(s_grid, scale), offset), tf.uint8)\n",
        "    enc = tf.image.encode_jpeg(s_grid)\n",
        "\n",
        "    fwrite = tf.write_file(name, enc)\n",
        "    return fwrite\n",
        "\n",
        "\n",
        "def main():\n",
        "    # initialize the data\n",
        "    mnist = input_data.read_data_sets('/tmp/MNIST_data')\n",
        "\n",
        "    # placeholders for the images\n",
        "    x = tf.placeholder(tf.float32, shape=[None, 784])\n",
        "\n",
        "    # build the model\n",
        "    loss, output, latent = autoencoder(x)\n",
        "\n",
        "    # and we use the Adam Optimizer for training\n",
        "    train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
        "\n",
        "    # We want to use Tensorboard to visualize some stuff\n",
        "    writer, summary_op = create_summaries(loss, x, latent, output)\n",
        "\n",
        "    first_batch = mnist.train.next_batch(BATCH_SIZE)\n",
        "\n",
        "    # Run the training loop\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        sess.run(make_image(\"images/input.jpg\", x, [28, 28]), feed_dict={x : \n",
        "            first_batch[0]})\n",
        "        for i in range(int(200001)):\n",
        "            batch = mnist.train.next_batch(BATCH_SIZE)\n",
        "            feed = {x : batch[0]}\n",
        "            if i % 1000 == 0:\n",
        "                summary, train_loss = sess.run([summary_op, loss], feed_dict=feed)\n",
        "                print(\"step %d, training loss: %g\" % (i, train_loss))\n",
        "\n",
        "                writer.add_summary(summary, i)\n",
        "                writer.flush()\n",
        "\n",
        "            if i % 1000 == 0:\n",
        "                sess.run(make_image(\"images/output_%06i.jpg\" % i, output, [28, \n",
        "                    28]), feed_dict={x : first_batch[0]})\n",
        "\n",
        "            train_step.run(feed_dict=feed)\n",
        "\n",
        "        # Save latent space\n",
        "        pred = sess.run(latent, feed_dict={x : mnist.test._images})\n",
        "        pred = np.asarray(pred)\n",
        "        pred = np.reshape(pred, (mnist.test._num_examples, 2))\n",
        "        labels = np.reshape(mnist.test._labels, (mnist.test._num_examples, 1))\n",
        "        pred = np.hstack((pred, labels))\n",
        "        #if USE_RELU:\n",
        "        #    fname = \"latent_relu.csv\"\n",
        "        #else:\n",
        "        #    fname = \"latent_default.csv\"\n",
        "        np.savetxt(\"latent_default.csv\", pred)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sbSDJNyP1mDT",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "files.download(\"latent_default.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}